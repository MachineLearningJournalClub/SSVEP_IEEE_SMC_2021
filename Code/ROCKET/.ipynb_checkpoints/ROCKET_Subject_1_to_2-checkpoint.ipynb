{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV1PzIYMJx2d"
   },
   "source": [
    "## Import libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yxN1_yiJx2h",
    "outputId": "9e948141-dde3-49c8-8859-c5a0a272f51f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os             : Linux-5.4.0-80-generic-x86_64-with-debian-bullseye-sid\n",
      "python         : 3.7.3\n",
      "tsai           : 0.2.22\n",
      "fastai         : 2.5.2\n",
      "fastcore       : 1.3.26\n",
      "torch          : 1.9.1+cu102\n",
      "n_cpus         : 10\n",
      "device         : cuda (Tesla T4)\n"
     ]
    }
   ],
   "source": [
    "# run it twice if it doesn't work\n",
    "from tsai.all import *\n",
    "computer_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5tj2jrbJx2j"
   },
   "source": [
    "## How to use the original ROCKET method? üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5mqPFeMJx2j"
   },
   "source": [
    "ROCKET is applied in 2 phases:\n",
    "\n",
    "1. Generate features from each time series: ROCKET calculates 20k features from each time series, independently of the sequence length. \n",
    "2. Apply a classifier to those calculated features. Those features are then used by the classifier of your choice. In the original code they use 2 simple linear classifiers: RidgeClassifierCV and Logistic Regression, but you can use any classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeYi0bumJx2l"
   },
   "source": [
    "### 1Ô∏è‚É£ Generate features\n",
    "\n",
    "Let's first generate the features. We'll import data from a UCR Time Series dataset.\n",
    "\n",
    "The original method requires the time series to be in a 2d array of shape (samples, len). Remember than only univariate sequences are allow in this original method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 256\n",
    "data_p1 = np.load(\"../../Pre-Processing/trials/subject_1_session_1_filt_ica_car.npy\")\n",
    "data_p2 = np.load(\"../../Pre-Processing/trials/subject_1_session_2_filt_ica_car.npy\")\n",
    "\n",
    "data_s1 = np.concatenate((data_p1, data_p2), axis = 0)\n",
    "labels_s1 = np.array([0,1,2,3]*10)\n",
    "\n",
    "data_s1 = data_s1[:,:,:time]\n",
    "\n",
    "data_p1_2 = np.load(\"../../Pre-Processing/trials/subject_2_session_1_filt_ica_car.npy\")\n",
    "data_p2_2 = np.load(\"../../Pre-Processing/trials/subject_2_session_2_filt_ica_car.npy\")\n",
    "\n",
    "data_s2 = np.concatenate((data_p1_2, data_p2_2), axis = 0)\n",
    "labels_s2 = np.array([0,1,2,3]*10)\n",
    "\n",
    "data_s2 = data_s2[:,:,:time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_vagSfxJx20"
   },
   "source": [
    "## How to use ROCKET with large and/ or multivariate datasets on GPU? - Recommended ‚≠êÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buff4iFUJx20"
   },
   "source": [
    "As stated before, the current ROCKET method doesn't support multivariate time series or GPU. This may be a drawback in some cases. \n",
    "\n",
    "To overcome both limitations I've created a multivariate ROCKET on GPU in Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCdQ2ASvJx20"
   },
   "source": [
    "### 1Ô∏è‚É£ Generate features\n",
    "\n",
    "First you prepare the input data and normalize it per sample. The input to ROCKET Pytorch is a 3d tensor of shape (samples, vars, len), preferrable on gpu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_6BoepTJx21"
   },
   "source": [
    "The way to use ROCKET in Pytorch is the following:\n",
    "\n",
    "* Create a dataset as you would normally do in `tsai`. \n",
    "* Create a TSDataLoaders with the following kwargs: \n",
    "    * drop_last=False. In this way we get features for every input sample.\n",
    "    * shuffle_train=False\n",
    "    * batch_tfms=[TSStandardize(by_sample=True)] so that input is normalized by sample, as recommended by the authors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNN5nfhuJx21"
   },
   "outputs": [],
   "source": [
    "#X, y, splits = get_UCR_data('HandMovementDirection', split_data=False)\n",
    "#splits = RandomSplitter()(range_of(data_p1))\n",
    "\n",
    "#data_p1 = np.load(\"../Pre-Processing/trials/subject_2_session_1_filt_ica_car.npy\")\n",
    "#labels_p1 = np.array([0,1,2, 3]*5)\n",
    "\n",
    "#data_p2 = np.load(\"../Pre-Processing/trials/subject_2_session_2_filt_ica_car.npy\")\n",
    "#labels_p2 = np.array([0,1,2, 3]*5)\n",
    "\n",
    "splits_1 = TrainValidTestSplitter(stratify = True, random_state= 10, valid_size = 0.)(range_of(data_s1))\n",
    "splits_2 = TrainValidTestSplitter(stratify = True, random_state= 0, valid_size = 0.)(range_of(data_s2))\n",
    "\n",
    "splits = (splits_1[0], splits_2[0])\n",
    "for i in range(0,40):\n",
    "    splits[1][i] += 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s12 = np.concatenate((data_s1, data_s2), axis = 0)\n",
    "labels_s12 = np.array([0,1,2,3]*20)\n",
    "# np.random.shuffle(labels_s12) #shuffling to see whether the model is learning or not \n",
    "data_s12 = data_s12[:,:,:time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms  = [None, [Categorize()]]\n",
    "batch_tfms = [TSStandardize(by_sample=True)]\n",
    "dls = get_ts_dls(data_s12, labels_s12,splits = splits, tfms=tfms, drop_last=False, \n",
    "                 shuffle_train=True, batch_tfms=batch_tfms, bs=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCRtLM6lJx22"
   },
   "source": [
    "‚ò£Ô∏è‚ò£Ô∏è You will be able to create a dls (TSDataLoaders) object with unusually large batch sizes. I've tested it with a large dataset and a batch size = 100_000 and it worked fine. This is because ROCKET is not a usual Deep Learning model. It just applies convolutions (kernels) one at a time to create the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ybz7wcJJx22"
   },
   "source": [
    "Instantiate a rocket model with the desired n_kernels (authors use 10_000) and kernel sizes (7, 9 and 11 in the original paper). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiEZp77FJx22"
   },
   "outputs": [],
   "source": [
    "model = build_ts_model(ROCKET, dls=dls, n_kernels = 20000, kss = [7, 9, 11]) # n_kernels=10_000, kss=[7, 9, 11] set by default, but you can pass other values as kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amKiHAeQJx23"
   },
   "source": [
    "Now generate rocket features for the entire train and valid datasets using the create_rocket_features convenience function `create_rocket_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xd1vC5ZLJx23"
   },
   "source": [
    "And we now transform the original data, creating 20k features per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VkjGXQcJx24"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = create_rocket_features(dls.train, model)\n",
    "X_valid, y_valid = create_rocket_features(dls.valid, model)\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDoiE7xfJx25"
   },
   "source": [
    "### 2Ô∏è‚É£ Apply a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNOjstElJx25"
   },
   "source": [
    "Once you build the 20k features per sample, you can use them to train any classifier of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRc5aDdBJx26"
   },
   "source": [
    "#### RidgeClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7MO0HXYJx26"
   },
   "source": [
    "And now you apply a classifier of your choice. \n",
    "With RidgeClassifierCV in particular, there's no need to normalize the calculated features before passing them to the classifier, as it does it internally (if normalize is set to True as recommended by the authors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDtNH9OLJx26"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "ridge = RidgeClassifierCV(alphas=np.logspace(-8, 8, 17), normalize=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(f'alpha: {ridge.alpha_:.2E}  train: {ridge.score(X_train, y_train):.5f}  valid: {ridge.score(X_valid, y_valid):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging over 50 Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4172704bd7a249c285a175197be4414e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_scores = []\n",
    "time = 256\n",
    "n_kernels = 2000\n",
    "    \n",
    "for i in tqdm(range(0, 50)):\n",
    "    \n",
    "    data_p1 = np.load(\"../../Pre-Processing/trials/subject_1_session_1_filt_ica_car.npy\")\n",
    "    data_p2 = np.load(\"../../Pre-Processing/trials/subject_1_session_2_filt_ica_car.npy\")\n",
    "    data_s1 = np.concatenate((data_p1, data_p2), axis = 0)\n",
    "    data_s1 = data_s1[:,:,:time]\n",
    "\n",
    "    data_p1_2 = np.load(\"../../Pre-Processing/trials/subject_2_session_1_filt_ica_car.npy\")\n",
    "    data_p2_2 = np.load(\"../../Pre-Processing/trials/subject_2_session_2_filt_ica_car.npy\")\n",
    "    data_s2 = np.concatenate((data_p1_2, data_p2_2), axis = 0)\n",
    "    data_s2 = data_s2[:,:,:time]\n",
    "    \n",
    "    splits_1 = TrainValidTestSplitter(stratify = True, random_state= 1000 -i, valid_size = 0.)(range_of(data_s1))\n",
    "    splits_2 = TrainValidTestSplitter(stratify = True, random_state= 0 + i, valid_size = 0.)(range_of(data_s2))\n",
    "\n",
    "    splits = (splits_1[0], splits_2[0])\n",
    "    for i in range(0,40):\n",
    "        splits[1][i] += 40\n",
    "    \n",
    "    data_s12 = np.concatenate((data_s1, data_s2), axis = 0)\n",
    "    labels_s12 = np.array([0,1,2,3]*20)\n",
    "    data_s12 = data_s12[:,:,:time]\n",
    "    \n",
    "    tfms  = [None, [Categorize()]]\n",
    "    batch_tfms = [TSStandardize(by_sample=True)]\n",
    "    dls = get_ts_dls(data_s12, labels_s12,splits = splits, tfms=tfms, drop_last=False, \n",
    "                     shuffle_train=True, batch_tfms=batch_tfms, bs=10_000)\n",
    "    \n",
    "    model = build_ts_model(ROCKET, dls=dls, n_kernels = n_kernels, kss = [7, 9, 11]) \n",
    "    # n_kernels=10_000, kss=[7, 9, 11] set by default, but you can pass other values as kwargs\n",
    "    \n",
    "    X_train, y_train = create_rocket_features(dls.train, model)\n",
    "    X_valid, y_valid = create_rocket_features(dls.valid, model)\n",
    "    \n",
    "    ridge = RidgeClassifierCV(alphas=np.logspace(-8, 8, 17), normalize=True)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    valid_scores.append(ridge.score(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print(sum(valid_scores)/len(valid_scores), np.std(valid_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_ROCKET_a_new_SOTA_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
