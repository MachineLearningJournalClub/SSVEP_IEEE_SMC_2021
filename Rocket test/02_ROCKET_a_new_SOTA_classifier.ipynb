{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfGAJYuBJx2T"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/timeseriesAI/tsai/blob/master/tutorial_nbs/02_ROCKET_a_new_SOTA_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jV-PF7eJx2a"
   },
   "source": [
    "created by Ignacio Oguiza - email: timeseriesAI@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5CQvN7LJx2b"
   },
   "source": [
    "## Purpose üòá"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSq1XK_uJx2c"
   },
   "source": [
    "The purpose of this notebook is to introduce you to Rocket. \n",
    "\n",
    "ROCKET (RandOm Convolutional KErnel Transform) is a new Time Series Classification (TSC) method that has just been released (Oct 29th, 2019), and has achieved **state-of-the-art performance on the UCR univariate time series classification datasets, surpassing HIVE-COTE (the previous state of the art since 2017) in accuracy, with exceptional speed compared to other traditional DL methods.** \n",
    "\n",
    "To achieve these 2 things at once is **VERY IMPRESSIVE**. ROCKET is certainly a new TSC method you should try.\n",
    "\n",
    "Authors:\n",
    "Dempster, A., Petitjean, F., & Webb, G. I. (2019). ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels. arXiv preprint arXiv:1910.13051.\n",
    "\n",
    "[paper](https://arxiv.org/pdf/1910.13051)\n",
    "\n",
    "There are 2 main limitations to the original ROCKET method though:\n",
    "- Released code doesn't handle multivariate data\n",
    "- It doesn't run on a GPU, so it's slow when used with a large datasets\n",
    "\n",
    "In this notebook you will learn: \n",
    "- how you can use the original ROCKET method\n",
    "- you will also learn about a new ROCKET version I have developed in Pytorch, that handles both **univariate and multivariate** data, and uses **GPU**\n",
    "- you will see how you can integrate the ROCKET features with fastai or other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV1PzIYMJx2d"
   },
   "source": [
    "## Import libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQk3I_ALJx2e"
   },
   "outputs": [],
   "source": [
    "# ## NOTE: UNCOMMENT AND RUN THIS CELL IF YOU NEED TO INSTALL/ UPGRADE TSAI\n",
    "# stable = False # True: stable version in pip, False: latest version from github\n",
    "# if stable: \n",
    "#     !pip install tsai -U >> /dev/null\n",
    "# else:      \n",
    "#     !pip install git+https://github.com/timeseriesAI/tsai.git -U >> /dev/null\n",
    "# ## NOTE: REMEMBER TO RESTART (NOT RECONNECT/ RESET) THE KERNEL/ RUNTIME ONCE THE INSTALLATION IS FINISHED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yxN1_yiJx2h",
    "outputId": "9e948141-dde3-49c8-8859-c5a0a272f51f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os             : Linux-5.4.0-80-generic-x86_64-with-debian-bullseye-sid\n",
      "python         : 3.7.3\n",
      "tsai           : 0.2.22\n",
      "fastai         : 2.5.2\n",
      "fastcore       : 1.3.26\n",
      "torch          : 1.9.1+cu102\n",
      "n_cpus         : 10\n",
      "device         : cuda (Tesla T4)\n"
     ]
    }
   ],
   "source": [
    "from tsai.all import *\n",
    "computer_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5tj2jrbJx2j"
   },
   "source": [
    "## How to use the original ROCKET method? üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5mqPFeMJx2j"
   },
   "source": [
    "ROCKET is applied in 2 phases:\n",
    "\n",
    "1. Generate features from each time series: ROCKET calculates 20k features from each time series, independently of the sequence length. \n",
    "2. Apply a classifier to those calculated features. Those features are then used by the classifier of your choice. In the original code they use 2 simple linear classifiers: RidgeClassifierCV and Logistic Regression, but you can use any classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeYi0bumJx2l"
   },
   "source": [
    "### 1Ô∏è‚É£ Generate features\n",
    "\n",
    "Let's first generate the features. We'll import data from a UCR Time Series dataset.\n",
    "\n",
    "The original method requires the time series to be in a 2d array of shape (samples, len). Remember than only univariate sequences are allow in this original method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p1 = np.load(\"../Pre-Processing/trials/subject_1_session_1_ICA_CAR.npy\")\n",
    "labels_p1 = np.array([0,1,2, 3]*5)\n",
    "\n",
    "data_p2 = np.load(\"../Pre-Processing/trials/subject_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_vagSfxJx20"
   },
   "source": [
    "## How to use ROCKET with large and/ or multivariate datasets on GPU? - Recommended ‚≠êÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buff4iFUJx20"
   },
   "source": [
    "As stated before, the current ROCKET method doesn't support multivariate time series or GPU. This may be a drawback in some cases. \n",
    "\n",
    "To overcome both limitations I've created a multivariate ROCKET on GPU in Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCdQ2ASvJx20"
   },
   "source": [
    "### 1Ô∏è‚É£ Generate features\n",
    "\n",
    "First you prepare the input data and normalize it per sample. The input to ROCKET Pytorch is a 3d tensor of shape (samples, vars, len), preferrable on gpu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_6BoepTJx21"
   },
   "source": [
    "The way to use ROCKET in Pytorch is the following:\n",
    "\n",
    "* Create a dataset as you would normally do in `tsai`. \n",
    "* Create a TSDataLoaders with the following kwargs: \n",
    "    * drop_last=False. In this way we get features for every input sample.\n",
    "    * shuffle_train=False\n",
    "    * batch_tfms=[TSStandardize(by_sample=True)] so that input is normalized by sample, as recommended by the authors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dNN5nfhuJx21"
   },
   "outputs": [],
   "source": [
    "#X, y, splits = get_UCR_data('HandMovementDirection', split_data=False)\n",
    "#splits = RandomSplitter()(range_of(data_p1))\n",
    "\n",
    "splits = TrainValidTestSplitter(stratify = True, random_state= 0)(range_of(data_p1))\n",
    "tfms  = [None, [Categorize()]]\n",
    "batch_tfms = [TSStandardize(by_sample=True)]\n",
    "dls = get_ts_dls(data_p1, labels_p1,splits = splits, tfms=tfms, drop_last=False, shuffle_train=False, batch_tfms=batch_tfms, bs=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#16) [10,17,6,13,4,2,5,14,9,7...], (#4) [18,1,19,8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCRtLM6lJx22"
   },
   "source": [
    "‚ò£Ô∏è‚ò£Ô∏è You will be able to create a dls (TSDataLoaders) object with unusually large batch sizes. I've tested it with a large dataset and a batch size = 100_000 and it worked fine. This is because ROCKET is not a usual Deep Learning model. It just applies convolutions (kernels) one at a time to create the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ybz7wcJJx22"
   },
   "source": [
    "Instantiate a rocket model with the desired n_kernels (authors use 10_000) and kernel sizes (7, 9 and 11 in the original paper). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "oiEZp77FJx22"
   },
   "outputs": [],
   "source": [
    "model = build_ts_model(ROCKET, dls=dls, n_kernels = 20000, kss = [3, 5, 7, 9, 11, 13]) # n_kernels=10_000, kss=[7, 9, 11] set by default, but you can pass other values as kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amKiHAeQJx23"
   },
   "source": [
    "Now generate rocket features for the entire train and valid datasets using the create_rocket_features convenience function `create_rocket_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xd1vC5ZLJx23"
   },
   "source": [
    "And we now transform the original data, creating 20k features per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "4VkjGXQcJx24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 40000), (4, 40000))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = create_rocket_features(dls.train, model)\n",
    "X_valid, y_valid = create_rocket_features(dls.valid, model)\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDoiE7xfJx25"
   },
   "source": [
    "### 2Ô∏è‚É£ Apply a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNOjstElJx25"
   },
   "source": [
    "Once you build the 20k features per sample, you can use them to train any classifier of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRc5aDdBJx26"
   },
   "source": [
    "#### RidgeClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7MO0HXYJx26"
   },
   "source": [
    "And now you apply a classifier of your choice. \n",
    "With RidgeClassifierCV in particular, there's no need to normalize the calculated features before passing them to the classifier, as it does it internally (if normalize is set to True as recommended by the authors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XDtNH9OLJx26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1.00E-08  train: 1.00000  valid: 0.25000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "ridge = RidgeClassifierCV(alphas=np.logspace(-8, 8, 17), normalize=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(f'alpha: {ridge.alpha_:.2E}  train: {ridge.score(X_train, y_train):.5f}  valid: {ridge.score(X_valid, y_valid):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um7CpRA5Jx27"
   },
   "source": [
    "This result is amazing!! The previous state of the art (Inceptiontime) was .37837"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGZGbEUqJx27"
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JYDRRYyJx27"
   },
   "source": [
    "In the case of other classifiers (like Logistic Regression), the authors recommend a per-feature normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "J8cC4-W2Jx27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 eps: 1.00E-05  C: 1.00E-05  loss: 1.31095  train_acc: 1.00000  valid_acc: 0.25000\n",
      " 1 eps: 1.00E-05  C: 1.00E-04  loss: 1.02247  train_acc: 1.00000  valid_acc: 0.25000\n",
      " 2 eps: 1.00E-05  C: 1.00E-03  loss: 0.80026  train_acc: 1.00000  valid_acc: 0.25000\n",
      " 3 eps: 1.00E-05  C: 1.00E-02  loss: 0.75231  train_acc: 1.00000  valid_acc: 0.25000\n",
      " 4 eps: 1.00E-05  C: 1.00E-01  loss: 0.74485  train_acc: 1.00000  valid_acc: 0.25000\n",
      " 5 eps: 1.00E-05  C: 1.00E+00  loss: 0.74382  train_acc: 1.00000  valid_acc: 0.25000\n",
      " 6 eps: 1.00E-05  C: 1.00E+01  loss: 0.74369  train_acc: 1.00000  valid_acc: 0.25000\n",
      " 7 eps: 1.00E-05  C: 1.00E+02  loss: 0.74367  train_acc: 1.00000  valid_acc: 0.25000\n",
      " 8 eps: 1.00E-05  C: 1.00E+03  loss: 0.74367  train_acc: 1.00000  valid_acc: 0.25000\n",
      " 9 eps: 1.00E-05  C: 1.00E+04  loss: 0.74367  train_acc: 1.00000  valid_acc: 0.25000\n",
      "10 eps: 1.00E-05  C: 1.00E+05  loss: 0.74367  train_acc: 1.00000  valid_acc: 0.25000\n",
      "\n",
      "Best result:\n",
      "eps: 1.00E-05  C: 1.00E+02  train_loss: 0.74367  train_acc: 1.00000  valid_acc: 0.25000\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-5\n",
    "Cs = np.logspace(-5, 5, 11)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "best_loss = np.inf\n",
    "for i, C in enumerate(Cs):\n",
    "    f_mean = X_train.mean(axis=0, keepdims=True)\n",
    "    f_std = X_train.std(axis=0, keepdims=True) + eps  # epsilon to avoid dividing by 0\n",
    "    X_train_tfm2 = (X_train - f_mean) / f_std\n",
    "    X_valid_tfm2 = (X_valid - f_mean) / f_std\n",
    "    classifier = LogisticRegression(penalty='l2', C=C, n_jobs=-1)\n",
    "    classifier.fit(X_train_tfm2, y_train)\n",
    "    probas = classifier.predict_proba(X_train_tfm2)\n",
    "    loss = nn.CrossEntropyLoss()(torch.tensor(probas), torch.tensor(y_train)).item()\n",
    "    train_score = classifier.score(X_train_tfm2, y_train)\n",
    "    val_score = classifier.score(X_valid_tfm2, y_valid)\n",
    "    if loss < best_loss:\n",
    "        best_eps = eps\n",
    "        best_C = C\n",
    "        best_loss = loss\n",
    "        best_train_score = train_score\n",
    "        best_val_score = val_score\n",
    "    print('{:2} eps: {:.2E}  C: {:.2E}  loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
    "        i, eps, C, loss, train_score, val_score))\n",
    "print('\\nBest result:')\n",
    "print('eps: {:.2E}  C: {:.2E}  train_loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
    "        best_eps, best_C, best_loss, best_train_score, best_val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7249t7xfJx28"
   },
   "source": [
    "‚ò£Ô∏è Note: Epsilon has a large impact on the result. You can actually test several values to find the one that best fits your problem, but bear in mind you can only select C and epsilon based on train data!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcKurgSoJx28"
   },
   "source": [
    "##### RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nvxx2PfqJx28"
   },
   "source": [
    "One way to do this would be to perform a random search using several epsilon and C values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "diarLquRJx29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  eps: 1.00E-01  C: 1.00E+02  loss: 0.74367  train_acc: 1.00000  valid_acc: 0.50000\n",
      " 1  eps: 1.00E-01  C: 1.00E+01  loss: 0.74372  train_acc: 1.00000  valid_acc: 0.50000\n",
      " 2  eps: 1.00E-03  C: 1.00E-04  loss: 1.03652  train_acc: 1.00000  valid_acc: 0.25000\n",
      " 3  eps: 1.00E-05  C: 1.00E-02  loss: 0.75231  train_acc: 1.00000  valid_acc: 0.25000\n",
      " 4  eps: 1.00E+00  C: 1.00E+01  loss: 0.74374  train_acc: 1.00000  valid_acc: 0.50000\n",
      " 5  eps: 1.00E+00  C: 1.00E-05  loss: 1.36144  train_acc: 1.00000  valid_acc: 0.50000\n",
      " 6  eps: 1.00E+00  C: 1.00E+04  loss: 0.74367  train_acc: 1.00000  valid_acc: 0.50000\n",
      " 7  eps: 1.00E-08  C: 1.00E-01  loss: 0.74485  train_acc: 1.00000  valid_acc: 0.25000\n",
      " 8  eps: 1.00E-05  C: 1.00E-02  loss: 0.75231  train_acc: 1.00000  valid_acc: 0.25000\n",
      " 9  eps: 1.00E-04  C: 1.00E+00  loss: 0.74382  train_acc: 1.00000  valid_acc: 0.25000\n",
      "\n",
      "Best result:\n",
      "eps: 1.00E+00  C: 1.00E+04  train_loss: 0.74367  train_acc: 1.00000  valid_acc: 0.50000\n"
     ]
    }
   ],
   "source": [
    "n_tests = 10\n",
    "epss = np.logspace(-8, 0, 9)\n",
    "Cs = np.logspace(-5, 5, 11)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "best_loss = np.inf\n",
    "for i in range(n_tests):\n",
    "    eps = np.random.choice(epss)\n",
    "    C = np.random.choice(Cs)\n",
    "    f_mean = X_train.mean(axis=0, keepdims=True)\n",
    "    f_std = X_train.std(axis=0, keepdims=True) + eps  # epsilon\n",
    "    X_train_tfm2 = (X_train - f_mean) / f_std\n",
    "    X_valid_tfm2 = (X_valid - f_mean) / f_std\n",
    "    classifier = LogisticRegression(penalty='l2', C=C, n_jobs=-1)\n",
    "    classifier.fit(X_train_tfm2, y_train)\n",
    "    probas = classifier.predict_proba(X_train_tfm2)\n",
    "    loss = nn.CrossEntropyLoss()(torch.tensor(probas), torch.tensor(y_train)).item()\n",
    "    train_score = classifier.score(X_train_tfm2, y_train)\n",
    "    val_score = classifier.score(X_valid_tfm2, y_valid)\n",
    "    if loss < best_loss:\n",
    "        best_eps = eps\n",
    "        best_C = C\n",
    "        best_loss = loss\n",
    "        best_train_score = train_score\n",
    "        best_val_score = val_score\n",
    "    print('{:2}  eps: {:.2E}  C: {:.2E}  loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
    "        i, eps, C, loss, train_score, val_score))\n",
    "print('\\nBest result:')\n",
    "print('eps: {:.2E}  C: {:.2E}  train_loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
    "        best_eps, best_C, best_loss, best_train_score, best_val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcZyumRsJx2-"
   },
   "source": [
    "In general, the original method may be a bit faster than the GPU method, but for larger datasets, there's a great benefit in using the GPU version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmaAhVItJx2-"
   },
   "source": [
    "In addition to this, I have also run the code on the TSC UCR multivariate datasets (all the ones that don't contain nan values), and the results are also very good, beating the previous state-of-the-art in this category as well by a large margin. For example, ROCKET reduces InceptionTime errors by 26% on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ps5OAACJx3C"
   },
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "6j1ZG7vWJx3C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:44:20] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:44:21] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[15:44:23] ../src/c_api/c_api_utils.h:161: Invalid missing value: null\nStack trace:\n  [bt] (0) /opt/tljh/user/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7fccfd8c633f]\n  [bt] (1) /opt/tljh/user/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x9c811) [0x7fccfd8d1811]\n  [bt] (2) /opt/tljh/user/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x9cac0) [0x7fccfd8d1ac0]\n  [bt] (3) /opt/tljh/user/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterPredictFromDense+0x225) [0x7fccfd8b9005]\n  [bt] (4) /opt/tljh/user/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7fcebaed9ec0]\n  [bt] (5) /opt/tljh/user/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7fcebaed987d]\n  [bt] (6) /opt/tljh/user/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fcebbed6f7e]\n  [bt] (7) /opt/tljh/user/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x139b4) [0x7fcebbed79b4]\n  [bt] (8) /opt/tljh/user/bin/python(_PyObject_FastCallKeywords+0x49b) [0x561a151e5d2b]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-18c41777fa97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mvalidate_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0mbase_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m             \u001b[0miteration_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         )\n\u001b[1;32m   1217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m    824\u001b[0m                     \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m                     \u001b[0mbase_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m                     \u001b[0mvalidate_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m                 )\n\u001b[1;32m    828\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_cupy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   1855\u001b[0m                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1857\u001b[0;31m                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1858\u001b[0m                 )\n\u001b[1;32m   1859\u001b[0m             )\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [15:44:23] ../src/c_api/c_api_utils.h:161: Invalid missing value: null\nStack trace:\n  [bt] (0) /opt/tljh/user/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7fccfd8c633f]\n  [bt] (1) /opt/tljh/user/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x9c811) [0x7fccfd8d1811]\n  [bt] (2) /opt/tljh/user/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x9cac0) [0x7fccfd8d1ac0]\n  [bt] (3) /opt/tljh/user/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterPredictFromDense+0x225) [0x7fccfd8b9005]\n  [bt] (4) /opt/tljh/user/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7fcebaed9ec0]\n  [bt] (5) /opt/tljh/user/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7fcebaed987d]\n  [bt] (6) /opt/tljh/user/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fcebbed6f7e]\n  [bt] (7) /opt/tljh/user/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x139b4) [0x7fcebbed79b4]\n  [bt] (8) /opt/tljh/user/bin/python(_PyObject_FastCallKeywords+0x49b) [0x561a151e5d2b]\n\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6\n",
    "\n",
    "# normalize 'per feature'\n",
    "f_mean = X_train.mean(axis=0, keepdims=True)\n",
    "f_std = X_train.std(axis=0, keepdims=True) + eps\n",
    "X_train_norm = (X_train - f_mean) / f_std\n",
    "X_valid_norm = (X_valid - f_mean) / f_std\n",
    "\n",
    "import xgboost as xgb\n",
    "classifier = xgb.XGBClassifier(max_depth=3,\n",
    "                               learning_rate=0.1,\n",
    "                               n_estimators=100,\n",
    "                               verbosity=1,\n",
    "                               objective='binary:logistic',\n",
    "                               booster='gbtree',\n",
    "                               tree_method='auto',\n",
    "                               n_jobs=-1,\n",
    "                               gpu_id=default_device().index,\n",
    "                               gamma=0,\n",
    "                               min_child_weight=1,\n",
    "                               max_delta_step=0,\n",
    "                               subsample=.5,\n",
    "                               colsample_bytree=1,\n",
    "                               colsample_bylevel=1,\n",
    "                               colsample_bynode=1,\n",
    "                               reg_alpha=0,\n",
    "                               reg_lambda=1,\n",
    "                               scale_pos_weight=1,\n",
    "                               base_score=0.5,\n",
    "                               random_state=0,\n",
    "                               missing=None)\n",
    "\n",
    "classifier.fit(X_train_norm, y_train)\n",
    "preds = classifier.predict(X_valid_norm)\n",
    "(preds == y_valid).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noiz0wdIJx3D"
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGPN5GTaJx3D"
   },
   "source": [
    "ROCKET is a great method for TSC that has established a new level of performance both in terms of accuracy and time. It does it by successfully applying an approach quite different from the traditional DL approaches. The method uses 10k random kernels to generate features that are then classified by linear classifiers (although you may use a classifier of your choice).\n",
    "The original method has 2 limitations (lack of multivariate and lack of GPU support) that are overcome by the Pytorch implementation shared in this notebook.\n",
    "\n",
    "So this is all the code you need to train a state-of-the-art model using rocket and GPU in `tsai`:\n",
    "\n",
    "```\n",
    "X, y, splits = get_UCR_data('HandMovementDirection', return_split=False)\n",
    "tfms  = [None, [Categorize()]]\n",
    "batch_tfms = [TSStandardize(by_sample=True)]\n",
    "dsets = TSDatasets(X, y, tfms=tfms, splits=splits)\n",
    "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=64, drop_last=False, shuffle_train=False, batch_tfms=[TSStandardize(by_sample=True)])\n",
    "model = create_model(ROCKET, dls=dls)\n",
    "X_train, y_train = create_rocket_features(dls.train, model)\n",
    "X_valid, y_valid = create_rocket_features(dls.valid, model)\n",
    "ridge = RidgeClassifierCV(alphas=np.logspace(-8, 8, 17), normalize=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(f'alpha: {ridge.alpha_:.2E}  train: {ridge.score(X_train, y_train):.5f}  valid: {ridge.score(X_valid, y_valid):.5f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eyh0d4GJ4Pk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_ROCKET_a_new_SOTA_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
