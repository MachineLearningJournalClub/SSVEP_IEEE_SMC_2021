{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "02_ROCKET_a_new_SOTA_classifier.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfGAJYuBJx2T"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timeseriesAI/tsai/blob/master/tutorial_nbs/02_ROCKET_a_new_SOTA_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jV-PF7eJx2a"
      },
      "source": [
        "created by Ignacio Oguiza - email: timeseriesAI@gmail.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2i8lAODKANw",
        "outputId": "ff30d3f9-6bff-4f85-e79b-323a87c51359",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install tsai"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tsai\n",
            "  Using cached tsai-0.2.22-py3-none-any.whl (180 kB)\n",
            "Collecting numba<0.54,>=0.53\n",
            "  Downloading numba-0.53.1-cp37-cp37m-manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 7.7 MB/s \n",
            "\u001b[?25hCollecting torch-optimizer>=0.1.0\n",
            "  Using cached torch_optimizer-0.1.0-py3-none-any.whl (72 kB)\n",
            "Collecting pyts>=0.11.0\n",
            "  Downloading pyts-0.11.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 37.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from tsai) (21.1.3)\n",
            "Collecting sktime>=0.7.0\n",
            "  Downloading sktime-0.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 9.0 MB/s \n",
            "\u001b[?25hCollecting scipy>=1.5\n",
            "  Downloading scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 50 kB/s \n",
            "\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tsai) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tsai) (21.0)\n",
            "Requirement already satisfied: torch<1.10,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from tsai) (1.9.0+cu111)\n",
            "Collecting tsfresh>=0.18.0\n",
            "  Downloading tsfresh-0.18.0-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.24.2\n",
            "  Using cached scikit_learn-1.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.1 MB)\n",
            "Collecting imbalanced-learn>=0.8.0\n",
            "  Downloading imbalanced_learn-0.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[K     |████████████████████████████████| 189 kB 78.2 MB/s \n",
            "\u001b[?25hCollecting fastai>=2.5.2\n",
            "  Using cached fastai-2.5.2-py3-none-any.whl (186 kB)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.2->tsai) (0.10.0+cu111)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.2->tsai) (1.0.0)\n",
            "Collecting fastcore<1.4,>=1.3.8\n",
            "  Downloading fastcore-1.3.26-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.2->tsai) (2.23.0)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.2->tsai) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.2->tsai) (3.2.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.2->tsai) (3.13)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.2->tsai) (2.2.4)\n",
            "Collecting fastdownload<2,>=0.0.5\n",
            "  Downloading fastdownload-0.0.5-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai>=2.5.2->tsai) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress>=0.2.4->fastai>=2.5.2->tsai) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn>=0.8.0->tsai) (1.0.1)\n",
            "Collecting llvmlite<0.37,>=0.36.0rc1\n",
            "  Downloading llvmlite-0.36.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 58 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<0.54,>=0.53->tsai) (57.4.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting statsmodels>=0.12.1\n",
            "  Downloading statsmodels-0.13.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 23.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from sktime>=0.7.0->tsai) (0.37.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai>=2.5.2->tsai) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai>=2.5.2->tsai) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->fastai>=2.5.2->tsai) (1.15.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.2->tsai) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.2->tsai) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.2->tsai) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.2->tsai) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.2->tsai) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.2->tsai) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.2->tsai) (4.62.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.2->tsai) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.2->tsai) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.5.2->tsai) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai>=2.5.2->tsai) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai>=2.5.2->tsai) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai>=2.5.2->tsai) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.5.2->tsai) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.5.2->tsai) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.5.2->tsai) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.5.2->tsai) (2021.5.30)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.1->sktime>=0.7.0->tsai) (0.5.2)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: dask[dataframe]>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh>=0.18.0->tsai) (2.12.0)\n",
            "Collecting stumpy>=1.7.2\n",
            "  Downloading stumpy-1.9.2-py3-none-any.whl (113 kB)\n",
            "\u001b[K     |████████████████████████████████| 113 kB 58.4 MB/s \n",
            "\u001b[?25hCollecting matrixprofile>=1.1.10<2.0.0\n",
            "  Downloading matrixprofile-1.1.10-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 35.6 MB/s \n",
            "\u001b[?25hCollecting distributed>=2.11.0\n",
            "  Downloading distributed-2021.9.1-py3-none-any.whl (786 kB)\n",
            "\u001b[K     |████████████████████████████████| 786 kB 46.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]>=2.9.0->tsfresh>=0.18.0->tsai) (0.11.1)\n",
            "Collecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 61.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai) (2.0.0)\n",
            "Collecting cloudpickle>=1.5.0\n",
            "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai) (2.4.0)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai) (5.4.8)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai) (1.7.0)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai) (5.1.1)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai) (1.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai) (2.11.3)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh>=0.18.0->tsai) (7.1.2)\n",
            "Collecting distributed>=2.11.0\n",
            "  Downloading distributed-2021.9.0-py3-none-any.whl (779 kB)\n",
            "\u001b[K     |████████████████████████████████| 779 kB 44.8 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.8.1-py3-none-any.whl (778 kB)\n",
            "\u001b[K     |████████████████████████████████| 778 kB 49.1 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.8.0-py3-none-any.whl (776 kB)\n",
            "\u001b[K     |████████████████████████████████| 776 kB 48.5 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.7.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 53.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.7.1-py3-none-any.whl (766 kB)\n",
            "\u001b[K     |████████████████████████████████| 766 kB 46.7 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.7.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 59.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.2-py3-none-any.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 25.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.1-py3-none-any.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 52.0 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.0-py3-none-any.whl (715 kB)\n",
            "\u001b[K     |████████████████████████████████| 715 kB 44.5 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.5.1-py3-none-any.whl (705 kB)\n",
            "\u001b[K     |████████████████████████████████| 705 kB 46.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.5.0-py3-none-any.whl (699 kB)\n",
            "\u001b[K     |████████████████████████████████| 699 kB 32.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.4.1-py3-none-any.whl (696 kB)\n",
            "\u001b[K     |████████████████████████████████| 696 kB 43.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.4.0-py3-none-any.whl (684 kB)\n",
            "\u001b[K     |████████████████████████████████| 684 kB 47.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.3.1-py3-none-any.whl (679 kB)\n",
            "\u001b[K     |████████████████████████████████| 679 kB 45.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.3.0-py3-none-any.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 43.0 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.2.0-py3-none-any.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 27.1 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.1.1-py3-none-any.whl (672 kB)\n",
            "\u001b[K     |████████████████████████████████| 672 kB 43.0 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.1.0-py3-none-any.whl (671 kB)\n",
            "\u001b[K     |████████████████████████████████| 671 kB 43.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2020.12.0-py3-none-any.whl (669 kB)\n",
            "\u001b[K     |████████████████████████████████| 669 kB 37.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2.30.1-py3-none-any.whl (656 kB)\n",
            "\u001b[K     |████████████████████████████████| 656 kB 50.1 MB/s \n",
            "\u001b[?25hCollecting protobuf==3.11.2\n",
            "  Downloading protobuf-3.11.2-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 43.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.5.2->tsai) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.5.2->tsai) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.5.2->tsai) (2.4.7)\n",
            "Collecting locket\n",
            "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.11.0->tsfresh>=0.18.0->tsai) (1.0.1)\n",
            "Installing collected packages: locket, llvmlite, threadpoolctl, scipy, protobuf, partd, numba, fsspec, fastcore, cloudpickle, stumpy, statsmodels, scikit-learn, pytorch-ranger, matrixprofile, fastdownload, distributed, tsfresh, torch-optimizer, sktime, pyts, imbalanced-learn, fastai, tsai\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.4.3\n",
            "    Uninstalling imbalanced-learn-0.4.3:\n",
            "      Successfully uninstalled imbalanced-learn-0.4.3\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.2.0 requires protobuf<4,>=3.13, but you have protobuf 3.11.2 which is incompatible.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\n",
            "googleapis-common-protos 1.53.0 requires protobuf>=3.12.0, but you have protobuf 3.11.2 which is incompatible.\n",
            "google-api-core 1.26.3 requires protobuf>=3.12.0, but you have protobuf 3.11.2 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed cloudpickle-2.0.0 distributed-2.30.1 fastai-2.5.2 fastcore-1.3.26 fastdownload-0.0.5 fsspec-2021.10.1 imbalanced-learn-0.8.1 llvmlite-0.36.0 locket-0.2.1 matrixprofile-1.1.10 numba-0.53.1 partd-1.2.0 protobuf-3.11.2 pytorch-ranger-0.1.1 pyts-0.11.0 scikit-learn-1.0 scipy-1.7.1 sktime-0.8.0 statsmodels-0.13.0 stumpy-1.9.2 threadpoolctl-3.0.0 torch-optimizer-0.1.0 tsai-0.2.22 tsfresh-0.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5CQvN7LJx2b"
      },
      "source": [
        "## Purpose 😇"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSq1XK_uJx2c"
      },
      "source": [
        "The purpose of this notebook is to introduce you to Rocket. \n",
        "\n",
        "ROCKET (RandOm Convolutional KErnel Transform) is a new Time Series Classification (TSC) method that has just been released (Oct 29th, 2019), and has achieved **state-of-the-art performance on the UCR univariate time series classification datasets, surpassing HIVE-COTE (the previous state of the art since 2017) in accuracy, with exceptional speed compared to other traditional DL methods.** \n",
        "\n",
        "To achieve these 2 things at once is **VERY IMPRESSIVE**. ROCKET is certainly a new TSC method you should try.\n",
        "\n",
        "Authors:\n",
        "Dempster, A., Petitjean, F., & Webb, G. I. (2019). ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels. arXiv preprint arXiv:1910.13051.\n",
        "\n",
        "[paper](https://arxiv.org/pdf/1910.13051)\n",
        "\n",
        "There are 2 main limitations to the original ROCKET method though:\n",
        "- Released code doesn't handle multivariate data\n",
        "- It doesn't run on a GPU, so it's slow when used with a large datasets\n",
        "\n",
        "In this notebook you will learn: \n",
        "- how you can use the original ROCKET method\n",
        "- you will also learn about a new ROCKET version I have developed in Pytorch, that handles both **univariate and multivariate** data, and uses **GPU**\n",
        "- you will see how you can integrate the ROCKET features with fastai or other classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV1PzIYMJx2d"
      },
      "source": [
        "## Import libraries 📚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQk3I_ALJx2e"
      },
      "source": [
        "# ## NOTE: UNCOMMENT AND RUN THIS CELL IF YOU NEED TO INSTALL/ UPGRADE TSAI\n",
        "# stable = False # True: stable version in pip, False: latest version from github\n",
        "# if stable: \n",
        "#     !pip install tsai -U >> /dev/null\n",
        "# else:      \n",
        "#     !pip install git+https://github.com/timeseriesAI/tsai.git -U >> /dev/null\n",
        "# ## NOTE: REMEMBER TO RESTART (NOT RECONNECT/ RESET) THE KERNEL/ RUNTIME ONCE THE INSTALLATION IS FINISHED"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yxN1_yiJx2h",
        "outputId": "9e948141-dde3-49c8-8859-c5a0a272f51f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tsai.all import *\n",
        "computer_setup()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "os             : Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\n",
            "python         : 3.7.12\n",
            "tsai           : 0.2.22\n",
            "fastai         : 2.5.2\n",
            "fastcore       : 1.3.26\n",
            "torch          : 1.9.0+cu111\n",
            "n_cpus         : 2\n",
            "device         : cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5tj2jrbJx2j"
      },
      "source": [
        "## How to use the original ROCKET method? 🚀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5mqPFeMJx2j"
      },
      "source": [
        "ROCKET is applied in 2 phases:\n",
        "\n",
        "1. Generate features from each time series: ROCKET calculates 20k features from each time series, independently of the sequence length. \n",
        "2. Apply a classifier to those calculated features. Those features are then used by the classifier of your choice. In the original code they use 2 simple linear classifiers: RidgeClassifierCV and Logistic Regression, but you can use any classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeYi0bumJx2l"
      },
      "source": [
        "### 1️⃣ Generate features\n",
        "\n",
        "Let's first generate the features. We'll import data from a UCR Time Series dataset.\n",
        "\n",
        "The original method requires the time series to be in a 2d array of shape (samples, len). Remember than only univariate sequences are allow in this original method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3CwE635Jx2m"
      },
      "source": [
        "X_train, y_train, X_valid, y_valid = get_UCR_data('OliveOil')\n",
        "seq_len = X_train.shape[-1]\n",
        "X_train = X_train[:, 0]\n",
        "X_valid = X_valid[:, 0]\n",
        "labels = np.unique(y_train)\n",
        "transform = {}\n",
        "for i, l in enumerate(labels): transform[l] = i\n",
        "y_train = np.vectorize(transform.get)(y_train)\n",
        "y_valid = np.vectorize(transform.get)(y_valid)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeBakgWSJx2p"
      },
      "source": [
        "Now we normalize the data to mean 0 and std 1 **'per sample'** (recommended by the authors), that is they set each sample to mean 0 and std 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhgug9HXJx2q",
        "outputId": "2608f628-5122-4117-f042-090d2c5b5feb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train = (X_train - X_train.mean(axis = 1, keepdims = True)) / (X_train.std(axis = 1, keepdims = True) + 1e-8)\n",
        "X_valid = (X_valid - X_valid.mean(axis = 1, keepdims = True)) / (X_valid.std(axis = 1, keepdims = True) + 1e-8)\n",
        "X_train.mean(axis = 1, keepdims = True).shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssj5W7WOJx2s"
      },
      "source": [
        "To generate the features, we first need to create the 10k random kernels that will be used to process the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZziwEWNJx2t"
      },
      "source": [
        "kernels = generate_kernels(seq_len, 10000)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go2cdebwJx2u"
      },
      "source": [
        "Now we apply those ramdom kernels to the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P7JcSSIJx2u"
      },
      "source": [
        "X_train_tfm = apply_kernels(X_train, kernels)\n",
        "X_valid_tfm = apply_kernels(X_valid, kernels)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byere7_eJx2v"
      },
      "source": [
        "### 2️⃣ Apply a classifier\n",
        "\n",
        "So now we have the features, and we are ready to apply a classifier. \n",
        "\n",
        "Let's use a simple, linear RidgeClassifierCV as they propose in the paper. We first instantiate it. \n",
        "\n",
        "Note:\n",
        "alphas: Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to C^-1 in other linear models such as LogisticRegression or LinearSVC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmEtOHBqJx2w"
      },
      "source": [
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 7), normalize=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob6gSpWIJx2x",
        "outputId": "9aacb984-2285-4bd0-d96b-5343130f490b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        " classifier.fit(X_train_tfm, y_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RidgeClassifierCV(alphas=array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
              "                  normalize=True)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS8TVWLzJx2y",
        "outputId": "80e69c2a-3f29-45a5-e2b2-f20c8e3c15a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "classifier.score(X_valid_tfm, y_valid)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn4A-2xSJx2y"
      },
      "source": [
        "☣️ **This is pretty impressive! It matches or exceeds the state-of-the-art performance without any fine tuning in <2 seconds!!!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbS3-_t7Jx2y",
        "outputId": "6af10334-582e-4260-e946-88891c8128dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "kernels = generate_kernels(seq_len, 10000)\n",
        "X_train_tfm = apply_kernels(X_train, kernels)\n",
        "X_valid_tfm = apply_kernels(X_valid, kernels)\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 7), normalize=True)\n",
        "classifier.fit(X_train_tfm, y_train)\n",
        "classifier.score(X_valid_tfm, y_valid)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmsumVBQJx2z"
      },
      "source": [
        "⚠️ Bear in mind that this process is not deterministic since there is randomness involved in the kernels. In thiis case, performance may vary between .9 to .933"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_vagSfxJx20"
      },
      "source": [
        "## How to use ROCKET with large and/ or multivariate datasets on GPU? - Recommended ⭐️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buff4iFUJx20"
      },
      "source": [
        "As stated before, the current ROCKET method doesn't support multivariate time series or GPU. This may be a drawback in some cases. \n",
        "\n",
        "To overcome both limitations I've created a multivariate ROCKET on GPU in Pytorch. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCdQ2ASvJx20"
      },
      "source": [
        "### 1️⃣ Generate features\n",
        "\n",
        "First you prepare the input data and normalize it per sample. The input to ROCKET Pytorch is a 3d tensor of shape (samples, vars, len), preferrable on gpu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_6BoepTJx21"
      },
      "source": [
        "The way to use ROCKET in Pytorch is the following:\n",
        "\n",
        "* Create a dataset as you would normally do in `tsai`. \n",
        "* Create a TSDataLoaders with the following kwargs: \n",
        "    * drop_last=False. In this way we get features for every input sample.\n",
        "    * shuffle_train=False\n",
        "    * batch_tfms=[TSStandardize(by_sample=True)] so that input is normalized by sample, as recommended by the authors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNN5nfhuJx21"
      },
      "source": [
        "X, y, splits = get_UCR_data('HandMovementDirection', split_data=False)\n",
        "tfms  = [None, [Categorize()]]\n",
        "batch_tfms = [TSStandardize(by_sample=True)]\n",
        "dls = get_ts_dls(X, y, splits=splits, tfms=tfms, drop_last=False, shuffle_train=False, batch_tfms=batch_tfms, bs=10_000)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCRtLM6lJx22"
      },
      "source": [
        "☣️☣️ You will be able to create a dls (TSDataLoaders) object with unusually large batch sizes. I've tested it with a large dataset and a batch size = 100_000 and it worked fine. This is because ROCKET is not a usual Deep Learning model. It just applies convolutions (kernels) one at a time to create the features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ybz7wcJJx22"
      },
      "source": [
        "Instantiate a rocket model with the desired n_kernels (authors use 10_000) and kernel sizes (7, 9 and 11 in the original paper). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiEZp77FJx22"
      },
      "source": [
        "model = build_ts_model(ROCKET, dls=dls) # n_kernels=10_000, kss=[7, 9, 11] set by default, but you can pass other values as kwargs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amKiHAeQJx23"
      },
      "source": [
        "Now generate rocket features for the entire train and valid datasets using the create_rocket_features convenience function `create_rocket_features`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd1vC5ZLJx23"
      },
      "source": [
        "And we now transform the original data, creating 20k features per sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VkjGXQcJx24"
      },
      "source": [
        "X_train, y_train = create_rocket_features(dls.train, model)\n",
        "X_valid, y_valid = create_rocket_features(dls.valid, model)\n",
        "X_train.shape, X_valid.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDoiE7xfJx25"
      },
      "source": [
        "### 2️⃣ Apply a classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNOjstElJx25"
      },
      "source": [
        "Once you build the 20k features per sample, you can use them to train any classifier of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRc5aDdBJx26"
      },
      "source": [
        "#### RidgeClassifierCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7MO0HXYJx26"
      },
      "source": [
        "And now you apply a classifier of your choice. \n",
        "With RidgeClassifierCV in particular, there's no need to normalize the calculated features before passing them to the classifier, as it does it internally (if normalize is set to True as recommended by the authors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDtNH9OLJx26"
      },
      "source": [
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "ridge = RidgeClassifierCV(alphas=np.logspace(-8, 8, 17), normalize=True)\n",
        "ridge.fit(X_train, y_train)\n",
        "print(f'alpha: {ridge.alpha_:.2E}  train: {ridge.score(X_train, y_train):.5f}  valid: {ridge.score(X_valid, y_valid):.5f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um7CpRA5Jx27"
      },
      "source": [
        "This result is amazing!! The previous state of the art (Inceptiontime) was .37837"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGZGbEUqJx27"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JYDRRYyJx27"
      },
      "source": [
        "In the case of other classifiers (like Logistic Regression), the authors recommend a per-feature normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8cC4-W2Jx27"
      },
      "source": [
        "eps = 1e-6\n",
        "Cs = np.logspace(-5, 5, 11)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "best_loss = np.inf\n",
        "for i, C in enumerate(Cs):\n",
        "    f_mean = X_train.mean(axis=0, keepdims=True)\n",
        "    f_std = X_train.std(axis=0, keepdims=True) + eps  # epsilon to avoid dividing by 0\n",
        "    X_train_tfm2 = (X_train - f_mean) / f_std\n",
        "    X_valid_tfm2 = (X_valid - f_mean) / f_std\n",
        "    classifier = LogisticRegression(penalty='l2', C=C, n_jobs=-1)\n",
        "    classifier.fit(X_train_tfm2, y_train)\n",
        "    probas = classifier.predict_proba(X_train_tfm2)\n",
        "    loss = nn.CrossEntropyLoss()(torch.tensor(probas), torch.tensor(y_train)).item()\n",
        "    train_score = classifier.score(X_train_tfm2, y_train)\n",
        "    val_score = classifier.score(X_valid_tfm2, y_valid)\n",
        "    if loss < best_loss:\n",
        "        best_eps = eps\n",
        "        best_C = C\n",
        "        best_loss = loss\n",
        "        best_train_score = train_score\n",
        "        best_val_score = val_score\n",
        "    print('{:2} eps: {:.2E}  C: {:.2E}  loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
        "        i, eps, C, loss, train_score, val_score))\n",
        "print('\\nBest result:')\n",
        "print('eps: {:.2E}  C: {:.2E}  train_loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
        "        best_eps, best_C, best_loss, best_train_score, best_val_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7249t7xfJx28"
      },
      "source": [
        "☣️ Note: Epsilon has a large impact on the result. You can actually test several values to find the one that best fits your problem, but bear in mind you can only select C and epsilon based on train data!!! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcKurgSoJx28"
      },
      "source": [
        "##### RandomSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvxx2PfqJx28"
      },
      "source": [
        "One way to do this would be to perform a random search using several epsilon and C values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diarLquRJx29"
      },
      "source": [
        "n_tests = 10\n",
        "epss = np.logspace(-8, 0, 9)\n",
        "Cs = np.logspace(-5, 5, 11)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "best_loss = np.inf\n",
        "for i in range(n_tests):\n",
        "    eps = np.random.choice(epss)\n",
        "    C = np.random.choice(Cs)\n",
        "    f_mean = X_train.mean(axis=0, keepdims=True)\n",
        "    f_std = X_train.std(axis=0, keepdims=True) + eps  # epsilon\n",
        "    X_train_tfm2 = (X_train - f_mean) / f_std\n",
        "    X_valid_tfm2 = (X_valid - f_mean) / f_std\n",
        "    classifier = LogisticRegression(penalty='l2', C=C, n_jobs=-1)\n",
        "    classifier.fit(X_train_tfm2, y_train)\n",
        "    probas = classifier.predict_proba(X_train_tfm2)\n",
        "    loss = nn.CrossEntropyLoss()(torch.tensor(probas), torch.tensor(y_train)).item()\n",
        "    train_score = classifier.score(X_train_tfm2, y_train)\n",
        "    val_score = classifier.score(X_valid_tfm2, y_valid)\n",
        "    if loss < best_loss:\n",
        "        best_eps = eps\n",
        "        best_C = C\n",
        "        best_loss = loss\n",
        "        best_train_score = train_score\n",
        "        best_val_score = val_score\n",
        "    print('{:2}  eps: {:.2E}  C: {:.2E}  loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
        "        i, eps, C, loss, train_score, val_score))\n",
        "print('\\nBest result:')\n",
        "print('eps: {:.2E}  C: {:.2E}  train_loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
        "        best_eps, best_C, best_loss, best_train_score, best_val_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcZyumRsJx2-"
      },
      "source": [
        "In general, the original method may be a bit faster than the GPU method, but for larger datasets, there's a great benefit in using the GPU version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmaAhVItJx2-"
      },
      "source": [
        "In addition to this, I have also run the code on the TSC UCR multivariate datasets (all the ones that don't contain nan values), and the results are also very good, beating the previous state-of-the-art in this category as well by a large margin. For example, ROCKET reduces InceptionTime errors by 26% on average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzd7bFYpJx2_"
      },
      "source": [
        "#### Fastai classifier head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovXJxhdhJx2_"
      },
      "source": [
        "X = concat(X_train, X_valid)\n",
        "y = concat(y_train, y_valid)\n",
        "splits = get_predefined_splits(X_train, X_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK4oUO-nJx2_"
      },
      "source": [
        "tfms  = [None, [Categorize()]]\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits)\n",
        "dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=128, batch_tfms=[TSStandardize(by_var=True)])# per feature normalization\n",
        "dls.show_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpJUGpybJx3A"
      },
      "source": [
        "def lin_zero_init(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        nn.init.constant_(layer.weight.data, 0.)\n",
        "        if layer.bias is not None: nn.init.constant_(layer.bias.data, 0.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BemLAciXJx3C"
      },
      "source": [
        "model = create_mlp_head(dls.vars, dls.c, dls.len)\n",
        "model.apply(lin_zero_init)\n",
        "learn = Learner(dls, model, metrics=accuracy, cbs=ShowGraph())\n",
        "learn.fit_one_cycle(50, lr_max=1e-4)\n",
        "learn.plot_metrics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ps5OAACJx3C"
      },
      "source": [
        "#### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j1ZG7vWJx3C"
      },
      "source": [
        "eps = 1e-6\n",
        "\n",
        "# normalize 'per feature'\n",
        "f_mean = X_train.mean(axis=0, keepdims=True)\n",
        "f_std = X_train.std(axis=0, keepdims=True) + eps\n",
        "X_train_norm = (X_train - f_mean) / f_std\n",
        "X_valid_norm = (X_valid - f_mean) / f_std\n",
        "\n",
        "import xgboost as xgb\n",
        "classifier = xgb.XGBClassifier(max_depth=3,\n",
        "                               learning_rate=0.1,\n",
        "                               n_estimators=100,\n",
        "                               verbosity=1,\n",
        "                               objective='binary:logistic',\n",
        "                               booster='gbtree',\n",
        "                               tree_method='auto',\n",
        "                               n_jobs=-1,\n",
        "                               gpu_id=default_device().index,\n",
        "                               gamma=0,\n",
        "                               min_child_weight=1,\n",
        "                               max_delta_step=0,\n",
        "                               subsample=.5,\n",
        "                               colsample_bytree=1,\n",
        "                               colsample_bylevel=1,\n",
        "                               colsample_bynode=1,\n",
        "                               reg_alpha=0,\n",
        "                               reg_lambda=1,\n",
        "                               scale_pos_weight=1,\n",
        "                               base_score=0.5,\n",
        "                               random_state=0,\n",
        "                               missing=None)\n",
        "\n",
        "classifier.fit(X_train_norm, y_train)\n",
        "preds = classifier.predict(X_valid_norm)\n",
        "(preds == y_valid).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noiz0wdIJx3D"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGPN5GTaJx3D"
      },
      "source": [
        "ROCKET is a great method for TSC that has established a new level of performance both in terms of accuracy and time. It does it by successfully applying an approach quite different from the traditional DL approaches. The method uses 10k random kernels to generate features that are then classified by linear classifiers (although you may use a classifier of your choice).\n",
        "The original method has 2 limitations (lack of multivariate and lack of GPU support) that are overcome by the Pytorch implementation shared in this notebook.\n",
        "\n",
        "So this is all the code you need to train a state-of-the-art model using rocket and GPU in `tsai`:\n",
        "\n",
        "```\n",
        "X, y, splits = get_UCR_data('HandMovementDirection', return_split=False)\n",
        "tfms  = [None, [Categorize()]]\n",
        "batch_tfms = [TSStandardize(by_sample=True)]\n",
        "dsets = TSDatasets(X, y, tfms=tfms, splits=splits)\n",
        "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=64, drop_last=False, shuffle_train=False, batch_tfms=[TSStandardize(by_sample=True)])\n",
        "model = create_model(ROCKET, dls=dls)\n",
        "X_train, y_train = create_rocket_features(dls.train, model)\n",
        "X_valid, y_valid = create_rocket_features(dls.valid, model)\n",
        "ridge = RidgeClassifierCV(alphas=np.logspace(-8, 8, 17), normalize=True)\n",
        "ridge.fit(X_train, y_train)\n",
        "print(f'alpha: {ridge.alpha_:.2E}  train: {ridge.score(X_train, y_train):.5f}  valid: {ridge.score(X_valid, y_valid):.5f}')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eyh0d4GJ4Pk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}