{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfGAJYuBJx2T"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/timeseriesAI/tsai/blob/master/tutorial_nbs/02_ROCKET_a_new_SOTA_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jV-PF7eJx2a"
   },
   "source": [
    "created by Ignacio Oguiza - email: timeseriesAI@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5CQvN7LJx2b"
   },
   "source": [
    "## Purpose üòá"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSq1XK_uJx2c"
   },
   "source": [
    "The purpose of this notebook is to introduce you to Rocket. \n",
    "\n",
    "ROCKET (RandOm Convolutional KErnel Transform) is a new Time Series Classification (TSC) method that has just been released (Oct 29th, 2019), and has achieved **state-of-the-art performance on the UCR univariate time series classification datasets, surpassing HIVE-COTE (the previous state of the art since 2017) in accuracy, with exceptional speed compared to other traditional DL methods.** \n",
    "\n",
    "To achieve these 2 things at once is **VERY IMPRESSIVE**. ROCKET is certainly a new TSC method you should try.\n",
    "\n",
    "Authors:\n",
    "Dempster, A., Petitjean, F., & Webb, G. I. (2019). ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels. arXiv preprint arXiv:1910.13051.\n",
    "\n",
    "[paper](https://arxiv.org/pdf/1910.13051)\n",
    "\n",
    "There are 2 main limitations to the original ROCKET method though:\n",
    "- Released code doesn't handle multivariate data\n",
    "- It doesn't run on a GPU, so it's slow when used with a large datasets\n",
    "\n",
    "In this notebook you will learn: \n",
    "- how you can use the original ROCKET method\n",
    "- you will also learn about a new ROCKET version I have developed in Pytorch, that handles both **univariate and multivariate** data, and uses **GPU**\n",
    "- you will see how you can integrate the ROCKET features with fastai or other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV1PzIYMJx2d"
   },
   "source": [
    "## Import libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQk3I_ALJx2e"
   },
   "outputs": [],
   "source": [
    "# ## NOTE: UNCOMMENT AND RUN THIS CELL IF YOU NEED TO INSTALL/ UPGRADE TSAI\n",
    "# stable = False # True: stable version in pip, False: latest version from github\n",
    "# if stable: \n",
    "#     !pip install tsai -U >> /dev/null\n",
    "# else:      \n",
    "#     !pip install git+https://github.com/timeseriesAI/tsai.git -U >> /dev/null\n",
    "# ## NOTE: REMEMBER TO RESTART (NOT RECONNECT/ RESET) THE KERNEL/ RUNTIME ONCE THE INSTALLATION IS FINISHED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yxN1_yiJx2h",
    "outputId": "9e948141-dde3-49c8-8859-c5a0a272f51f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os             : Linux-5.4.0-80-generic-x86_64-with-debian-bullseye-sid\n",
      "python         : 3.7.3\n",
      "tsai           : 0.2.22\n",
      "fastai         : 2.5.2\n",
      "fastcore       : 1.3.26\n",
      "torch          : 1.9.1+cu102\n",
      "n_cpus         : 10\n",
      "device         : cuda (Tesla T4)\n"
     ]
    }
   ],
   "source": [
    "from tsai.all import *\n",
    "computer_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5tj2jrbJx2j"
   },
   "source": [
    "## How to use the original ROCKET method? üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5mqPFeMJx2j"
   },
   "source": [
    "ROCKET is applied in 2 phases:\n",
    "\n",
    "1. Generate features from each time series: ROCKET calculates 20k features from each time series, independently of the sequence length. \n",
    "2. Apply a classifier to those calculated features. Those features are then used by the classifier of your choice. In the original code they use 2 simple linear classifiers: RidgeClassifierCV and Logistic Regression, but you can use any classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeYi0bumJx2l"
   },
   "source": [
    "### 1Ô∏è‚É£ Generate features\n",
    "\n",
    "Let's first generate the features. We'll import data from a UCR Time Series dataset.\n",
    "\n",
    "The original method requires the time series to be in a 2d array of shape (samples, len). Remember than only univariate sequences are allow in this original method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p1 = np.load(\"../Pre-Processing/trials/subject_1_session_1_filt_ica_car.npy\")\n",
    "labels_p1 = np.array([0,1,2, 3]*5)\n",
    "\n",
    "data_p2 = np.load(\"../Pre-Processing/trials/subject_1_session_2_filt_ica_car.npy\")\n",
    "labels_p2 = np.array([0,1,2, 3]*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p1 = np.concatenate((data_p1, data_p2), axis = 0)\n",
    "labels_p1 = np.array([0,1,2,3]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p1 = data_p1[:,:,:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 8, 256)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_p1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_vagSfxJx20"
   },
   "source": [
    "## How to use ROCKET with large and/ or multivariate datasets on GPU? - Recommended ‚≠êÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buff4iFUJx20"
   },
   "source": [
    "As stated before, the current ROCKET method doesn't support multivariate time series or GPU. This may be a drawback in some cases. \n",
    "\n",
    "To overcome both limitations I've created a multivariate ROCKET on GPU in Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCdQ2ASvJx20"
   },
   "source": [
    "### 1Ô∏è‚É£ Generate features\n",
    "\n",
    "First you prepare the input data and normalize it per sample. The input to ROCKET Pytorch is a 3d tensor of shape (samples, vars, len), preferrable on gpu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_6BoepTJx21"
   },
   "source": [
    "The way to use ROCKET in Pytorch is the following:\n",
    "\n",
    "* Create a dataset as you would normally do in `tsai`. \n",
    "* Create a TSDataLoaders with the following kwargs: \n",
    "    * drop_last=False. In this way we get features for every input sample.\n",
    "    * shuffle_train=False\n",
    "    * batch_tfms=[TSStandardize(by_sample=True)] so that input is normalized by sample, as recommended by the authors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "dNN5nfhuJx21"
   },
   "outputs": [],
   "source": [
    "#X, y, splits = get_UCR_data('HandMovementDirection', split_data=False)\n",
    "#splits = RandomSplitter()(range_of(data_p1))\n",
    "\n",
    "splits = TrainValidTestSplitter(stratify = True, random_state= 10, valid_size = 0.2)(range_of(data_p1))\n",
    "tfms  = [None, [Categorize()]]\n",
    "batch_tfms = [TSStandardize(by_sample=True)]\n",
    "dls = get_ts_dls(data_p1, labels_p1,splits = splits, tfms=tfms, drop_last=False, \n",
    "                 shuffle_train=True, batch_tfms=batch_tfms, bs=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#32) [31,12,5,6,3,21,20,34,1,18...], (#8) [2,27,35,30,14,13,7,24])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCRtLM6lJx22"
   },
   "source": [
    "‚ò£Ô∏è‚ò£Ô∏è You will be able to create a dls (TSDataLoaders) object with unusually large batch sizes. I've tested it with a large dataset and a batch size = 100_000 and it worked fine. This is because ROCKET is not a usual Deep Learning model. It just applies convolutions (kernels) one at a time to create the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ybz7wcJJx22"
   },
   "source": [
    "Instantiate a rocket model with the desired n_kernels (authors use 10_000) and kernel sizes (7, 9 and 11 in the original paper). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "oiEZp77FJx22"
   },
   "outputs": [],
   "source": [
    "model = build_ts_model(ROCKET, dls=dls, n_kernels = 20000, kss = [7, 9, 11]) # n_kernels=10_000, kss=[7, 9, 11] set by default, but you can pass other values as kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amKiHAeQJx23"
   },
   "source": [
    "Now generate rocket features for the entire train and valid datasets using the create_rocket_features convenience function `create_rocket_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xd1vC5ZLJx23"
   },
   "source": [
    "And we now transform the original data, creating 20k features per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "4VkjGXQcJx24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 40000), (8, 40000))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = create_rocket_features(dls.train, model)\n",
    "X_valid, y_valid = create_rocket_features(dls.valid, model)\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDoiE7xfJx25"
   },
   "source": [
    "### 2Ô∏è‚É£ Apply a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNOjstElJx25"
   },
   "source": [
    "Once you build the 20k features per sample, you can use them to train any classifier of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRc5aDdBJx26"
   },
   "source": [
    "#### RidgeClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7MO0HXYJx26"
   },
   "source": [
    "And now you apply a classifier of your choice. \n",
    "With RidgeClassifierCV in particular, there's no need to normalize the calculated features before passing them to the classifier, as it does it internally (if normalize is set to True as recommended by the authors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "XDtNH9OLJx26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1.00E+01  train: 1.00000  valid: 1.00000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "ridge = RidgeClassifierCV(alphas=np.logspace(-8, 8, 17), normalize=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "#print(f'alpha: {ridge.alpha_:.2E}  train: {ridge.score(X_train, y_train):.5f}  valid: {ridge.score(X_valid, y_valid):.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44f835cb1394b618d9ff750fcd5a91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_scores = []\n",
    "    \n",
    "for i in tqdm(range(0, 50)):\n",
    "    splits = TrainValidTestSplitter(stratify = True, random_state= i, valid_size = 0.2)(range_of(data_p1))\n",
    "    tfms  = [None, [Categorize()]]\n",
    "    batch_tfms = [TSStandardize(by_sample=True)]\n",
    "    dls = get_ts_dls(data_p1, labels_p1,splits = splits, tfms=tfms, drop_last=False, \n",
    "                     shuffle_train=True, batch_tfms=batch_tfms, bs=10_000)\n",
    "    \n",
    "    model = build_ts_model(ROCKET, dls=dls, n_kernels = 20000, kss = [7, 9, 11])\n",
    "    \n",
    "    X_train, y_train = create_rocket_features(dls.train, model)\n",
    "    X_valid, y_valid = create_rocket_features(dls.valid, model)\n",
    "    \n",
    "    ridge = RidgeClassifierCV(alphas=np.logspace(-8, 8, 17), normalize=True)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    valid_scores.append(ridge.score(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(valid_scores)/len(valid_scores), np.std(valid_scores)/len(valid_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um7CpRA5Jx27"
   },
   "source": [
    "This result is amazing!! The previous state of the art (Inceptiontime) was .37837"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGZGbEUqJx27"
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JYDRRYyJx27"
   },
   "source": [
    "In the case of other classifiers (like Logistic Regression), the authors recommend a per-feature normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "J8cC4-W2Jx27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 eps: 1.00E-05  C: 1.00E-05  loss: 1.26409  train_acc: 0.96875  valid_acc: 0.62500\n",
      " 1 eps: 1.00E-05  C: 1.00E-04  loss: 0.93947  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 2 eps: 1.00E-05  C: 1.00E-03  loss: 0.77945  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 3 eps: 1.00E-05  C: 1.00E-02  loss: 0.74896  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 4 eps: 1.00E-05  C: 1.00E-01  loss: 0.74438  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 5 eps: 1.00E-05  C: 1.00E+00  loss: 0.74376  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 6 eps: 1.00E-05  C: 1.00E+01  loss: 0.74368  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 7 eps: 1.00E-05  C: 1.00E+02  loss: 0.74367  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 8 eps: 1.00E-05  C: 1.00E+03  loss: 0.74367  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 9 eps: 1.00E-05  C: 1.00E+04  loss: 0.74367  train_acc: 1.00000  valid_acc: 0.75000\n",
      "10 eps: 1.00E-05  C: 1.00E+05  loss: 0.74367  train_acc: 1.00000  valid_acc: 0.75000\n",
      "\n",
      "Best result:\n",
      "eps: 1.00E-05  C: 1.00E+02  train_loss: 0.74367  train_acc: 1.00000  valid_acc: 0.75000\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-5\n",
    "Cs = np.logspace(-5, 5, 11)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "best_loss = np.inf\n",
    "for i, C in enumerate(Cs):\n",
    "    f_mean = X_train.mean(axis=0, keepdims=True)\n",
    "    f_std = X_train.std(axis=0, keepdims=True) + eps  # epsilon to avoid dividing by 0\n",
    "    X_train_tfm2 = (X_train - f_mean) / f_std\n",
    "    X_valid_tfm2 = (X_valid - f_mean) / f_std\n",
    "    classifier = LogisticRegression(penalty='l2', C=C, n_jobs=-1)\n",
    "    classifier.fit(X_train_tfm2, y_train)\n",
    "    probas = classifier.predict_proba(X_train_tfm2)\n",
    "    loss = nn.CrossEntropyLoss()(torch.tensor(probas), torch.tensor(y_train)).item()\n",
    "    train_score = classifier.score(X_train_tfm2, y_train)\n",
    "    val_score = classifier.score(X_valid_tfm2, y_valid)\n",
    "    if loss < best_loss:\n",
    "        best_eps = eps\n",
    "        best_C = C\n",
    "        best_loss = loss\n",
    "        best_train_score = train_score\n",
    "        best_val_score = val_score\n",
    "    print('{:2} eps: {:.2E}  C: {:.2E}  loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
    "        i, eps, C, loss, train_score, val_score))\n",
    "print('\\nBest result:')\n",
    "print('eps: {:.2E}  C: {:.2E}  train_loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
    "        best_eps, best_C, best_loss, best_train_score, best_val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7249t7xfJx28"
   },
   "source": [
    "‚ò£Ô∏è Note: Epsilon has a large impact on the result. You can actually test several values to find the one that best fits your problem, but bear in mind you can only select C and epsilon based on train data!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcKurgSoJx28"
   },
   "source": [
    "##### RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nvxx2PfqJx28"
   },
   "source": [
    "One way to do this would be to perform a random search using several epsilon and C values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "diarLquRJx29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  eps: 1.00E-05  C: 1.00E+03  loss: 0.74367  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 1  eps: 1.00E-02  C: 1.00E+04  loss: 0.74367  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 2  eps: 1.00E-03  C: 1.00E-05  loss: 1.26586  train_acc: 0.96875  valid_acc: 0.62500\n",
      " 3  eps: 1.00E-05  C: 1.00E-02  loss: 0.74896  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 4  eps: 1.00E-06  C: 1.00E+01  loss: 0.74368  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 5  eps: 1.00E-04  C: 1.00E-03  loss: 0.77949  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 6  eps: 1.00E-05  C: 1.00E-04  loss: 0.93947  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 7  eps: 1.00E-01  C: 1.00E+03  loss: 0.74367  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 8  eps: 1.00E-04  C: 1.00E-01  loss: 0.74438  train_acc: 1.00000  valid_acc: 0.75000\n",
      " 9  eps: 1.00E+00  C: 1.00E+00  loss: 0.74389  train_acc: 1.00000  valid_acc: 0.75000\n",
      "\n",
      "Best result:\n",
      "eps: 1.00E-01  C: 1.00E+03  train_loss: 0.74367  train_acc: 1.00000  valid_acc: 0.75000\n"
     ]
    }
   ],
   "source": [
    "n_tests = 10\n",
    "epss = np.logspace(-8, 0, 9)\n",
    "Cs = np.logspace(-5, 5, 11)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "best_loss = np.inf\n",
    "for i in range(n_tests):\n",
    "    eps = np.random.choice(epss)\n",
    "    C = np.random.choice(Cs)\n",
    "    f_mean = X_train.mean(axis=0, keepdims=True)\n",
    "    f_std = X_train.std(axis=0, keepdims=True) + eps  # epsilon\n",
    "    X_train_tfm2 = (X_train - f_mean) / f_std\n",
    "    X_valid_tfm2 = (X_valid - f_mean) / f_std\n",
    "    classifier = LogisticRegression(penalty='l2', C=C, n_jobs=-1)\n",
    "    classifier.fit(X_train_tfm2, y_train)\n",
    "    probas = classifier.predict_proba(X_train_tfm2)\n",
    "    loss = nn.CrossEntropyLoss()(torch.tensor(probas), torch.tensor(y_train)).item()\n",
    "    train_score = classifier.score(X_train_tfm2, y_train)\n",
    "    val_score = classifier.score(X_valid_tfm2, y_valid)\n",
    "    if loss < best_loss:\n",
    "        best_eps = eps\n",
    "        best_C = C\n",
    "        best_loss = loss\n",
    "        best_train_score = train_score\n",
    "        best_val_score = val_score\n",
    "    print('{:2}  eps: {:.2E}  C: {:.2E}  loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
    "        i, eps, C, loss, train_score, val_score))\n",
    "print('\\nBest result:')\n",
    "print('eps: {:.2E}  C: {:.2E}  train_loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
    "        best_eps, best_C, best_loss, best_train_score, best_val_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eyh0d4GJ4Pk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_ROCKET_a_new_SOTA_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
